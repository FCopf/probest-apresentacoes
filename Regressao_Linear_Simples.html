<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modelos Lineares Clássicos</title>
    <meta charset="utf-8" />
    <meta name="author" content="Fabio Cop (fabiocopf@gmail.com)" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Modelos Lineares Clássicos
]
.subtitle[
## Regressão Linear Simples
]
.author[
### Fabio Cop (<a href="mailto:fabiocopf@gmail.com" class="email">fabiocopf@gmail.com</a>)
]
.institute[
### Instituto do Mar - UNIFESP
]
.date[
### Última atualização em 06 de julho de 2022
]

---




&lt;style type="text/css"&gt;
    .h1_small h1 {
      font-weight: normal;
      margin-top: -75px;
      margin-left: -00px;
      color: #FAFAFA;
      font-size: 150%;
}

    .pull-left-min {
      float: left;
      width: 70%;
    }

    .pull-right-min {
      float: right;
      width: 27%;
    }

    .fundo_capa {
      background-image: url('img/Linear_regression_capa.jpg');
      background-size: 100%;
      background-position: center;
    }

    .fundo_esquema {
      background-image: url('img/Regressao_Linear_Correlacao_esquema.jpg');
      background-size: 95%;
      background-position: center;
    }    
    .regression table {
      font-size: 16px;
    }

    .regression_small table {
      font-size: 13px;
    }
&lt;/style&gt;



---

class: h1_small, fundo_capa

# Regressão Linear Simples

.pull-left[

.content-box-blue[

___

1. Medindo a intensidade de relações lineares 

1. Variâncias e covariâncias

1. Regressão Linear Simples

1. Teste de hipóteses

1. Intervalos de confiança e de predição

1. Partição da Soma dos Quadrados e variação explicada

1. Regressão Linear Simples: comandos no R

1. Pressupostos do modelo

1. Transformações lineares

___

]

]

---
class: h1_small

# Um  pouco de história

## Método dos Mínimos Quadrados (MMQ)

.pull-left[

#### A primeira solução para o problema da regressão (relacionar uma variável resposta `\(Y\)` a uma variável preditora `\(X\)`) foi o **Método dos Mínimos Quadrados (MMQ)**, publicado por por Gauss (1777 – 1855) em 1809, embora haja relatos históricos de que Gauss pensou e resolveu o problema quando tinha apenas 11 anos. Gauss aplicou o método para obter predições sobre as órbitas dos corpos ao redor do Sol a partir de observações astronômicas.

]

.pull-right[


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-2-1.png" width="365" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small


# Um  pouco de história

## O termo Regressão

.pull-left[

#### O termo **regressão** foi empregado por Francis Galton em 1866, um dos pais da Biometria e primo de *Charles Darwin*, no séc. XIX, para descrever o fenômeno biológico em que pais muito altos tenderiam a ter descendentes mais baixos que eles próprios e vice versa. A altura dos descendentes tenderia portanto a *regressar* à média da população.

]

.pull-right[


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-3-1.png" width="315" style="display: block; margin: auto;" /&gt;


]

---

class: h1_small

# Um  pouco de história

## O coeficiente de correlação de Pearson

.pull-left[


#### Galton propôs o **coeficiente de correlação** para medir a associação linear entre duas variáveis quantitativas. Suas idéias foram estendidas por Udny Yule e Karl Pearson para um contexto estatístico mais geral. No modelo de Udny e Pearson assume-se que a distribuição conjunta entre a variável resposta e a variável preditora `\(f(Y,X)\)` é Gaussiana (Normal). Pearson foi um dos diversos autores que começaram a utilizar o termo **distribuição Normal** no início do século `\(XX\)`.

]

.pull-right[


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-4-1.png" width="303" style="display: block; margin: auto;" /&gt;


]


---

class: h1_small

# Um  pouco de história

## A formulação de Fisher

.pull-left[

#### A suposição de Pearson confunde os conceitos de regressão e correlação. Esta suposição foi modificada por R. A. Fisher em 1922 e 1925. Fisher assumiu que a distribuição **condicional** da variável resposta `\(f(Y|X)\)` seja Gaussiana - a conjunta não precisa ser. Esta solução é mais próxima daquela formulada por Gauss. Fisher desenvolveu também o método da **Máxima Verossimilhança (MV)**. Para uma variável em que `\(f(Y|X)\)` é Gaussiana, a solução pelo **MMQ** e pela **MV** convergem. Fisher se dedicou também ao problema de encontrar uma distribuição estatística para o coeficiente de correlação de Pearson.

]

.pull-right[


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-5-1.png" width="529" style="display: block; margin: auto;" /&gt;


]

---

class: h1_small, fundo_esquema

# Sequência conceitual

---

class: h1_small

# 1. Medindo a intensidade de relações lineares

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small

# 2. Variâncias e Covariâncias




.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

____

&lt;h3 style="text-align:center"&gt;Soma dos Quadrados de \(Y\)&lt;/h3&gt;

___

`$$SQ_Y = \sum_{i = 1}^{n} (y_i - \overline{y})^2 = \sum_{i = 1}^{n} (y_i - \overline{y}) (y_i - \overline{y})$$`

&lt;br&gt;&lt;/br&gt;
____

&lt;h3 style="text-align:center"&gt;Variância amostral de \(Y\)&lt;/h3&gt;

___


`$$s^{2}_{Y} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y})^2}{n-1}$$`

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

____

&lt;h3 style="text-align:center"&gt;Soma dos Quadrados de \(X\)&lt;/h3&gt;

___

`$$SQ_X = \sum_{i = 1}^{n} (x_i - \overline{x})^2 = \sum_{i = 1}^{n} (x_i - \overline{x}) (x_i - \overline{x})$$`

&lt;br&gt;&lt;/br&gt;
____

&lt;h3 style="text-align:center"&gt;Variância amostral de \(X\)&lt;/h3&gt;

___

`$$s^{2}_{X} = \frac{\sum_{i = 1}^{n} (x_i - \overline{x})^2}{n-1}$$`

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

___

&lt;h3 style="text-align:center;font-size:140%;"&gt;Soma dos produtos cruzados de \(Y\) e \(X\)&lt;/h3&gt;

___

`$$SQ_{YX} = \sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})$$`

&lt;br&gt;&lt;/br&gt;

___

&lt;h3 style="text-align:center"&gt;Covariância amostral entre \(Y\) e \(X\)&lt;/h3&gt;

___

`$$s_{YX} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})}{n-1}$$`

]

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

### Se

`\((y_i - \overline{y}) &gt; 0\)`; `\((x_i - \overline{x}) &lt; 0\)`


### ou

`\((y_i - \overline{y}) &lt; 0\)`; `\((x_i - \overline{x}) &gt; 0\)`

### temos

`\(s_{YX} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})}{n-1} &lt; 0\)`

]

&lt;h3 style="text-align:center"&gt; A covariância pode ser &lt;font color="red"&gt;&lt;b&gt;NEGATIVA&lt;/b&gt;&lt;/font&gt;&lt;/h3&gt;

---

class: h1_small

# 2. Variâncias e Covariâncias

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

### Se

`\((y_i - \overline{y}) &gt; 0\)`; `\((x_i - \overline{x}) &gt; 0\)`

### ou

`\((y_i - \overline{y}) &lt; 0\)`; `\((x_i - \overline{x}) &lt; 0\)`

### temos

`\(s_{YX} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})}{n-1} &gt; 0\)`

]

&lt;h3 style="text-align:center"&gt; A covariância pode ser &lt;font color="red"&gt;&lt;b&gt;POSITIVA&lt;/b&gt;&lt;/font&gt;&lt;/h3&gt;


---

class: h1_small

# 2. Variâncias e Covariâncias


.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

### Se

`\((y_i - \overline{y}) \approx 0\)`; `\((x_i - \overline{x}) \approx 0\)`

### ou

`\((y_i - \overline{y}) \approx 0\)`; `\((x_i - \overline{x}) \approx 0\)`

### Temos

`\(s_{YX} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})}{n-1} \approx 0\)`

]

&lt;h3 style="text-align:center"&gt; A covariância pode ser &lt;font color="red"&gt;&lt;b&gt;NULA&lt;/b&gt;&lt;/font&gt;&lt;/h3&gt;

---

class: h1_small

# 2. Variâncias e Covariâncias

&lt;/br&gt;

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small, regression

# 3. Regressão linear simples: descrevendo relações funcionais

___

Um serviço ecossistêmico essencial de bacias hidrográficas é o fornecimento hídrico. Em 1955, o Serviço Florestal americano estabeleceu a Floresta Experimental de [**Hubbard Brook (HBEF)**](https://hubbardbrook.org/) como um centro de pesquisa hidrológica. 

Podemos supor que o volume de água anual que uma bacia pode fornecer tem relação com o volume de chuva na região.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small, regression

# 3. Regressão linear simples: descrevendo relações funcionais

___

Um serviço ecossistêmico essencial de bacias hidrográficas é o fornecimento hídrico. Em 1955, o Serviço Florestal americano estabeleceu a Floresta Experimental de [**Hubbard Brook (HBEF)**](https://hubbardbrook.org/) como um centro de pesquisa hidrológica. 

Podemos supor que o volume de água anual que uma bacia pode fornecer tem relação com o volume de chuva na região.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small, regression

# 3. Regressão linear simples: descrevendo relações funcionais

___

Um serviço ecossistêmico essencial de bacias hidrográficas é o fornecimento hídrico. Em 1955, o Serviço Florestal americano estabeleceu a Floresta Experimental de [**Hubbard Brook (HBEF)**](https://hubbardbrook.org/) como um centro de pesquisa hidrológica. 

Podemos supor que o volume de água anual que uma bacia pode fornecer tem relação com o volume de chuva na região.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Seja uma variável aleatória `\(Y\)` com distribuição normal proveniente de um *experimento aleatório*.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Seja uma variável aleatória `\(Y\)` com distribuição normal proveniente de um *experimento aleatório*.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Para cada observação `\(y_i\)` é conhecida também uma informação sobre `\(x_i\)`.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-24-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Para cada observação `\(y_i\)` é conhecida também uma informação sobre `\(x_i\)`.

___


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-25-1.gif" style="display: block; margin: auto;" /&gt;

---


class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Para cada observação `\(y_i\)` é conhecida também uma informação sobre `\(x_i\)`.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Para cada observação `\(y_i\)` é conhecida também uma informação sobre `\(x_i\)`.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

___

Para cada observação `\(y_i\)` é conhecida também uma informação sobre `\(x_i\)`.

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

.pull-left[

1 - As observações em `\(Y\)` e `\(X\)` compõem um par `\((y_i, x_i)\)` de modo que:

`$$Y = \begin{bmatrix} y_1 \\ y_2 \\ \cdots \\ y_n \end{bmatrix},
X = \begin{bmatrix} x_1 \\ x_2 \\ \cdots \\ x_n \end{bmatrix}$$`

2 - `\(X\)` é determinada **experimentalmente** e **sem erros**.

3 - `\(Y\)` é uma variável aleatória normalmente distribuída, com `\(\mu_i\)` variância `\(\sigma^2\)`.

`$$Y \sim \mathcal{N}(\mu_i, \sigma^2)$$`

]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 3. Regressão linear simples: estrutura geral do modelo

.pull-left[

4 - `\(\mu_i\)` é representado por um **modelo linear** que expressa o valor esperado de `\(y_i\)` para um dado valor de `\(x_i\)`. Compõe a **parcela determinística** do modelo.

`$$E(Y|x_i) = \mu_i = \beta_0 + \beta_1x_i$$`

5 - `\(\beta_0\)` e `\(\beta_1\)` são as contantes a serem estimadas, representando o **intercepto** e o **coeficience de inclinação da reta**, repectivamente.

6 - `\(\sigma^2\)` é a **variância** de `\(Y\)` e ser estimada. `\(\sigma^2\)` é **constante** para todos os valores em `\(X\)`.
]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-30-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 3. Regressão linear simples: o modelo matemático

.pull-left[

___

#### Variáveis envolvidas

___

`\(Y\)`: variável resposta (dependente);

`\(X\)`: variável preditora (**in**dependente);

`$$E(Y|x_i) = \beta_0 + \beta_1x_i$$`

___

Parâmetros do modelo

___

`\(\beta_0\)`: Intercepto;

`\(\beta_1\)`: coeficiente de inclinação da reta (**coeficiente de regressão**);

]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 3. Regressão linear simples: o modelo matemático

#### Se o intercepto `\(\beta_0\)` e a inclinação `\(\beta_1\)` são conhecidos, podemos **PREDIZER** qualquer valor `\(y_i\)` para um dado valor em `\(x_i\)`.

___

`$$E(Y|x_i) = \beta_0 + \beta_1x_i$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-32-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small, regression_small

# 3. Regressão linear simples: a tabela e o gráfico de dispersão

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[


&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(y_i\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 766.58 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 827.62 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 814.09 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 796.54 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 755.55 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 775.74 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 829.26 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9.81 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 811.61 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 710.96 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 699.34 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 13.70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 744.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 751.35 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 727.52 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 704.99 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 19.13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-34-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small, regression_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(y_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\hat{y}_i\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 766.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 804.59 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 827.62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 803.94 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 814.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 793.94 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 796.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 786.98 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 755.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 785.79 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 775.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 784.55 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 829.26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 780.76 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9.81 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 811.61 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 761.58 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 710.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 757.41 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 699.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 753.93 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 13.70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 744.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 739.88 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 751.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 738.11 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 727.52 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.20 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 704.99 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 714.06 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 19.13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 709.64 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-36-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small, regression_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(y_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\hat{y}_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\epsilon_i\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 766.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 804.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -38.01 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 827.62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 803.94 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23.68 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 814.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 793.94 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.15 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 796.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 786.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.56 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 755.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 785.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -30.25 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 775.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 784.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.82 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 829.26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 780.76 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 48.50 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 9.81 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 811.61 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 761.58 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 50.03 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 710.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 757.41 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -46.45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 699.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 753.93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -54.59 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 13.70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 744.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 739.88 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.34 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 751.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 738.11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 727.52 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.32 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 704.99 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 714.06 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -9.06 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 19.13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 709.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.36 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-38-1.png" style="display: block; margin: auto;" /&gt;

`\(\varepsilon_i\)`: resíduo - responsável pela variação de `\(y_i\)` em torno do valor **predito** `\((\hat{y}_i)\)` pela reta de regressão.

]

---

class: h1_small, regression_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

&lt;table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(y_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\hat{y}_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\epsilon_i\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 766.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 804.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -38.01 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 827.62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 803.94 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23.68 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 814.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 793.94 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.15 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 796.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 786.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.56 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 755.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 785.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -30.25 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 775.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 784.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.82 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 829.26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 780.76 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 48.50 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9.81 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 811.61 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 761.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50.03 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 710.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 757.41 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -46.45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 11.18 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 699.34 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; 753.93 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #23373B !important;"&gt; -54.59 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 13.70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 744.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 739.88 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.34 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 751.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 738.11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 727.52 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.32 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 704.99 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 714.06 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -9.06 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 19.13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 726.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 709.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.36 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-40-1.png" style="display: block; margin: auto;" /&gt;

`\(\varepsilon_i\)`: resíduo - responsável pela variação de `\(y_i\)` em torno do valor **predito** `\((\hat{y}_i)\)` pela reta de regressão.

]

---

class: h1_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

### O resíduo associado a cada observação diminui ou aumenta à medida que o ponto está mais próximo ou distante da reta de regressão.

___

Assume-se que os resíduos têm distribuição Normal de probabilidades com média zero e variância `\(\sigma^2\)`.

`$$\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$$`


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-41-1.png" style="display: block; margin: auto;" /&gt;

### `\(\sigma^2\)` elevada 

- pontos distantes da reta

]

---

class: h1_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

### O resíduo associado a cada observação diminui ou aumenta à medida que o ponto está mais próximo ou distante da reta de regressão.

___
Assume-se que os resíduos têm distribuição Normal de probabilidades com média zero e variância `\(\sigma^2\)`.

`$$\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$$`

]


.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-42-1.png" style="display: block; margin: auto;" /&gt;

#### `\(\sigma^2\)` reduzida 

- pontos próximos da reta

]

---

class: h1_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[


Variáveis e quantias envolvidas

___

- `\(y_i\)`: variável resposta - `\(i\)`: `\(1 \cdots n\)`;

- `\(x_i\)`: variável preditora - `\(i\)`: `\(1 \cdots n\)`;

- `\(n\)`: tamanho da amostra;

___

Parâmetros do modelo

___

- `\(\beta_0\)`: intercepto;

- `\(\beta_1\)`: coeficiente inclinação da reta;

- `\(\sigma^2\)`: variância do resíduo.

]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-43-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 3. Regressão linear simples: o modelo estatístico

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

.pull-left[

#### Parte determinística: `\(\beta_0\)` e `\(\beta_1\)`

___

`$$\beta_0 + \beta_1x_i$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-44-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

#### Parte estocástica: `\(\sigma^2\)`

___

`$$\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-45-1.png" style="display: block; margin: auto;" /&gt;

]


---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-46-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-47-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-48-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

`$$y_i|x_i = \beta_0 + \beta_1x_i + \varepsilon_i$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-49-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

#### O Método dos Mínimos Quadrados 

___

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-50-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-rigth[

#### Soma dos quadrados dos resíduos `\((SQ_{Res})\)`

___

`$$SQ_{Res} = \sum_{i=1}^{n}\varepsilon_i^2 = \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$`

O método dos mínimos quadrados consiste em encontrar a reta que **MINIMIZA** o somatório dos quadrados dos resíduos.

___

]

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

#### O Método dos Mínimos Quadrados 

___


.pull-left[


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-rigth[

#### Soma dos quadrados dos resíduos `\((SQ_{Res})\)`

___

`$$SQ_{Res} = \sum_{i=1}^{n}\varepsilon_i^2 = \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$`

O método dos mínimos quadrados consiste em encontrar a reta que **MINIMIZA** o somatório dos quadrados dos resíduos.

___

]

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

#### O Método dos Mínimos Quadrados 

___

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-52-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-rigth[

#### Soma dos quadrados dos resíduos `\((SQ_{Res})\)`

___

`$$SQ_{Res} = \sum_{i=1}^{n}\varepsilon_i^2 = \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$`

O método dos mínimos quadrados consiste em encontrar a reta que **MINIMIZA** o somatório dos quadrados dos resíduos.

___

]

---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

#### O Método dos Mínimos Quadrados

___

`$$SQ_{Res} = \sum_{i=1}^{n}\varepsilon_i^2 = \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-53-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small

# 3. Regressão linear simples: estimativa dos parâmetros

___

#### ---&gt; *Estime* `\(\hat{\beta}_0\)` e `\(\hat{\beta}_1\)` que minimize a quantia:

___

`$$\sum_{i=1}^{n}(y_i - \hat{y_i})^2 = \sum_{i=1}^{n}(y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-54-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: relembrando da Covariância

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-55-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

___

&lt;h3 style="text-align:center;font-size:140%;"&gt;Soma dos produtos cruzados de \(Y\) e \(X\)&lt;/h3&gt;

___

`$$SQ_{YX} = \sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})$$`

&lt;br&gt;&lt;/br&gt;

___

&lt;h3 style="text-align:center"&gt;Covariância amostral entre \(Y\) e \(X\)&lt;/h3&gt;

___

`$$s_{YX} = \frac{\sum_{i = 1}^{n} (y_i - \overline{y}) (x_i - \overline{x})}{n-1}$$`

]

---

class: h1_small

# 3. Regressão linear simples: estimando `\(\beta_1\)`

___

#### `$$\hat{\beta}_1 = \frac{SQ_{YX}}{SQ_X} = \frac{s_{XY}}{s^2_X}$$`

___


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-56-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small

# 3. Regressão linear simples: estimando `\(\beta_0\)`
___

#### `$$\overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}$$`

#### `$$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$`

___


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-57-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 3. Regressão linear simples: estimando `\(\sigma^2\)`
___

#### `$$y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + \varepsilon_i$$`

___

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-58-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[

#### O Quadrado Médio do Resíduo `\((QM_{Res})\)`

___

`$$s^2 = QM_{Res} = \frac{\sum_{i=1}^{n}{(Y_i - \hat{Y})^2}}{n-2}$$`

___

]


---

class: h1_small

# 4. Teste de hipóteses

.pull-left[

___

#### Hipótese nula

___

#### `$$H_0: \beta_1 = 0$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-59-1.png" style="display: block; margin: auto;" /&gt;

`$$y_i = \beta_0 + \varepsilon_i$$`
]

.pull-right[

___

#### Hipótese alternativa

___

#### `$$H_1: \beta_1 \ne 0$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-60-1.png" style="display: block; margin: auto;" /&gt;

`$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$`
]

---

class: h1_small

# 4. Teste de hipóteses

___

`\(H_0\)` pode ser testada por meio do teste `\(t\)` para o estimador `\(\hat{\beta}_1\)`

`\(t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}\)`; `\(s_{\hat{\beta}_1} = \sqrt{\frac{s^2}{SQ_{X}}}\)`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-61-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende da **magnitude de `\(\hat{\beta_1}\)`**

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-62-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende da **magnitude de `\(\hat{\beta_1}\)`**

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-63-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende da **magnitude de `\(\hat{\beta_1}\)`**

`$$t_{calculado} = \frac{\hat{\beta}_1 - \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-64-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende da **variância residual** - `\(s_{\hat{\beta_1}} = \sqrt{\frac{s^2}{SQ_{X}}}\)`

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-65-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende da **variância residual** - `\(s_{\hat{\beta_1}} = \sqrt{\frac{s^2}{SQ_{X}}}\)`

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-66-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende do **tamanho da amostra** - `\(n\)`

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-67-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

___

`\(t_{calculado}\)` depende do **tamanho da amostra** - `\(n\)`

`$$t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}}$$`

___

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-68-1.png" style="display: block; margin: auto;" /&gt;

---

class: h1_small

# 4. Teste de hipóteses

.pull-left[



___

Na figura ao lado, os coeficientes de regressão foram estimados pelo MMQ em `\(\hat{\beta}_0 = -568.55\)` e `\(\hat{\beta}_1 = 1.05\)`.

O valor de `\(t\)` foi:

`\(t_{calculado} = \frac{\hat{\beta}_1- \beta_1}{s_{\hat{\beta}_1}} = \frac{1.05 - 0}{0.06} = 17.451\)`

___

O valor de `\(p &lt; 0.001\)` associado a este resultado, se interpretado ao nível de significância `\(\alpha = 0,05\)`, é dito estatísticamente significativo, o que  nos leva a **rejeitar** `\(H_0\)`.

A conclusão é de que **existe** uma relação crescente entre a Precipitação e a Vazão na bacia hidrográfica.

]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-70-1.png" style="display: block; margin: auto;" /&gt;
]


---

class: h1_small

# 5. Intervalo de confiança para `\(\hat{Y}\)`

.pull-left[

Cada repetição do experimento com amostra de tamanho `\(n\)` irá resultar em diferentes valores de `\(\hat{Y}\)` e consequentemente diferentes **retas de regressão**. O erro padrão de `\(\hat{Y}\)` é dado por:

___

`$$s_{\hat{Y}|X} = \sqrt{s^2\left(\frac{1}{n} + \frac{(X_i - \overline{X})^2}{SQ_X}\right)}$$`
___

O intervalo de confiância de `\(\hat{Y}\)` é dado por:

___

`$$\hat{Y} \pm t_{(\alpha, n-2)}s_{\hat{Y}|X}$$`
___

#### A confiança para `\(\hat{Y}\)` aumenta ao redor de `\(\overline{X}\)` e diminui nos extremos da distribuição de `\(X_i\)`

]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-71-1.png" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small

# 5. Intervalo de predição para `\(Y^*\)`

.pull-left[

Tendo um modelo de regressão ajustado, o  que esperar para `\(Y\)` se obtivermos **um novo dado** em `\(X^*\)`? O erro padrão de `\(Y^*\)` é dado por:

___

`$$s_{Y^*|X^*} = \sqrt{s^2\left(1 + \frac{1}{n} + \frac{(X^* - \overline{X})^2}{SQ_X}\right)}$$`
___

O intervalo de **predição** de `\(Y^*\)` é dado por:

___

`$$Y^* \pm t_{(\alpha, n-2)}s_{Y^*|X^*}$$`
___

]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-72-1.png" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small

# 5. Intervalo de confiança *vs* intervalo de predição

.pull-left[

Intervalo de confiança de `\(\hat{Y}\)`
___

`$$s_{\hat{Y}|X} = \sqrt{s^2\left(\frac{1}{n} + \frac{(X_i - \overline{X})^2}{SQ_X}\right)}$$`

`$$\hat{Y} \pm t_{(\alpha, n-2)}s_{\hat{Y}|X}$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-73-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

Intervalo de predição de `\(Y_i^*\)`
___

`$$s_{Y^*|X^*} = \sqrt{s^2\left(1 + \frac{1}{n} + \frac{(X^* - \overline{X})^2}{SQ_X}\right)}$$`

`$$Y^* \pm t_{(\alpha, n-2)}s_{Y^*|X^*}$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-74-1.png" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small

# 5. Intervalos de confiança para `\(\hat{\beta_0}\)` e `\(\hat{\beta_1}\)`

.pull-left[

Para `\(X_i = 0\)`, `\(\hat{Y} = \hat{\beta_0}\)` de modo que:
___

`$$s_{\hat{\beta_0}} = \sqrt{s^2\left(\frac{1}{n} + \frac{\overline{X}^2}{SQ_X}\right)}$$`
___

O intervalo de confiância de `\(\hat{\beta_0}\)` é dado por:

___

`$$\hat{\beta_0} \pm t_{(\alpha, n-2)}s_{\hat{\beta_0}}$$`
___


]

.pull-right[

Para `\(\hat{\beta_1}\)` temos:
___

`$$s_{\hat{\beta_1}} = \sqrt{\left(\frac{s^2}{SQ_X}\right)}$$`
___

O intervalo de confiância de `\(\hat{\beta_1}\)` é dado por:

___

`$$\hat{\beta_1} \pm t_{(\alpha, n-2)}s_{\hat{\beta_1}}$$`
___

]

---


class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada

&lt;p&gt;&lt;h4 style="text-align:center"&gt;Somatório dos Quadrados da Regressão - \(SQ_{Reg}\)&lt;/h4&gt;&lt;/p&gt;

___

.pull-left[

`$$SQ_{Reg} = \sum_{i=1}^{n}{(\hat{Y}_i-\overline{Y})^2}$$`


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-75-1.png" style="display: block; margin: auto;" /&gt;

]

---


class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada

&lt;p&gt;&lt;h4 style="text-align:center"&gt;Somatório dos Quadrados do Resíduo - \(SQ_{Res}\)&lt;/h4&gt;&lt;/p&gt;

___

.pull-left[

`$$SQ_{Res} = \sum_{i=1}^{n}{(Y_i-\hat{Y}_i)^2}$$`


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-76-1.png" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada

&lt;p&gt;&lt;h4 style="text-align:center"&gt;Somatório dos Quadrados de \(Y\) - \(SQ_{Y}\)&lt;/h4&gt;&lt;/p&gt;

___

.pull-left[

`$$SQ_{Y} = \sum_{i=1}^{n}{(Y_i-\overline{Y})^2}$$`


]

.pull-right[
&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-77-1.png" style="display: block; margin: auto;" /&gt;

]
---

class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada

`$$SQ_Y = SQ_{Reg} + SQ_{Res}$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-78-1.png" style="display: block; margin: auto;" /&gt;


---

class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada


&lt;h3 style="text-align:center"&gt;O Coeficiente de Determinação&lt;/h3&gt;

____

`$$R^2 = \frac{SQ_{Reg}}{SQ_{Y}} = \frac{SQ_{Reg}}{SQ_{Reg} + SQ_{Res}}$$`

____

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-79-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

### `\(R^2\)` estima a proporção da variação em `\(Y\)` que pode ser atribuída ao modelo de regressão linear.

`$$0 \le R^2 \le 1$$`

&gt; Numericamente, o `\(R^2\)` é igual ao coeficiente de correlação linear de Pearson elevado ao quadrado.

]


---
class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada


&lt;h3 style="text-align:center"&gt;A Análise de Variância da Regressão&lt;/h3&gt;

____

&lt;br&gt;&lt;/br&gt;

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; gl &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SQ &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; QM &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; F &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9294.914 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9294.914 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 77.49312 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8e-07 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Resíduo &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1559.285 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 119.945 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

+ `\(gl\)`: graus de liberdade

+ `\(SQ\)`: soma dos quadrados

+ `\(QM\)`: quadrado médio

+ `\(F\)`: estatística `\(F\)`

+ `\(p\)`: valor de probabilidade na distribuição `\(F\)`

____

---

class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada


&lt;h3 style="text-align:center"&gt;A Análise de Variância da Regressão&lt;/h3&gt;

____

`$$F_{calculado} = \frac{QM_{Reg}}{QM_{Res}}$$`

____

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-81-1.png" style="display: block; margin: auto;" /&gt;



]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-82-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 6. Partição da Soma dos Quadrados e variação explicada


&lt;h3 style="text-align:center"&gt;A Análise de Variância da Regressão&lt;/h3&gt;

____

`$$F_{calculado} = \frac{QM_{Reg}}{QM_{Res}}$$`

____

.pull-left[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-83-1.png" style="display: block; margin: auto;" /&gt;



]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-84-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small, code70

# 7. Regressão Linear Simples: comandos no R

.pull-left[


```r
m_regressao = lm(Flow ~ Precipitation , data = st_ref)
summary(m_regressao)
```

```
## 
## Call:
## lm(formula = Flow ~ Precipitation, data = st_ref)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -87.86 -41.68 -14.03  30.64 153.09 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -568.54529   78.88794  -7.207  6.2e-08 ***
## Precipitation    1.05130    0.06024  17.451  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 61.51 on 29 degrees of freedom
## Multiple R-squared:  0.9131,	Adjusted R-squared:  0.9101 
## F-statistic: 304.5 on 1 and 29 DF,  p-value: &lt; 2.2e-16
```

]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-86-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small, code70

# 7. Regressão Linear Simples: comandos no R

.pull-left[


```r
anova(m_regressao)
```

```
## Analysis of Variance Table
## 
## Response: Flow
##               Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Precipitation  1 1152275 1152275  304.53 &lt; 2.2e-16 ***
## Residuals     29  109730    3784                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-88-1.png" style="display: block; margin: auto;" /&gt;
]



---

class: h1_small

# 8. Pressupostos do modelo

___

Ao realizar uma regressão linear simples, devemos assumir/testar alguns pressupostos.

___

.pull-left[

1. &lt;span style="color:#ed5858"&gt;O modelo linear descreve adequadamente a relação funcional entre `\(Y\)` e `\(X\)`;&lt;/span&gt;

2. Cada par de observação `\((y_i,x_i)\)` é independente dos demais;

3. A variável `\(X\)` é medida sem erros;

4. Os resíduos têm distribuição normal;

5. A variância residual `\(\sigma^2\)` é constante ao longo de `\(X\)`.
]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-89-1.png" style="display: block; margin: auto;" /&gt;

]

---

class: h1_small

# 8. Pressupostos do modelo

___

Ao realizar uma regressão linear simples, devemos assumir/testar alguns pressupostos.

___

.pull-left[

1. O modelo linear descreve adequadamente a relação funcional entre `\(Y\)` e `\(X\)`;

2. &lt;span style="color:#ed5858"&gt;Cada par de observação `\((y_i,x_i)\)` é independente dos demais;&lt;/span&gt;

3. A variável `\(X\)` é medida sem erros;

4. Os resíduos têm distribuição normal;

5. A variância residual `\(\sigma^2\)` é constante ao longo de `\(X\)`.
]

.pull-right[

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-90-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 8. Pressupostos do modelo

___

Ao realizar uma regressão linear simples, devemos assumir/testar alguns pressupostos.

___

.pull-left[

1. O modelo linear descreve adequadamente a relação funcional entre `\(Y\)` e `\(X\)`;

2. Cada par de observação `\((y_i,x_i)\)` é independente dos demais;

3. &lt;span style="color:#ed5858"&gt;A variável `\(X\)` é medida sem erros;&lt;/span&gt;

4. Os resíduos têm distribuição normal;

5. A variância residual `\(\sigma^2\)` é constante ao longo de `\(X\)`.
]

.pull-right[

`$$Y \sim \mathcal{N}(\mu_i, \sigma^2)$$`

`$$E(Y|x_i) = \mu_i = \beta_0 + \beta_1x_i$$`

&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-91-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 8. Pressupostos do modelo

___

Ao realizar uma regressão linear simples, devemos assumir/testar alguns pressupostos.

___

.pull-left[

1. O modelo linear descreve adequadamente a relação funcional entre `\(Y\)` e `\(X\)`;

2. Cada par de observação `\((y_i,x_i)\)` é independente dos demais;

3. A variável `\(X\)` é medida sem erros;

4. &lt;span style="color:#ed5858"&gt;Os resíduos têm distribuição normal;&lt;/span&gt;

5. A variância residual `\(\sigma^2\)` é constante ao longo de `\(X\)`.
]

.pull-right[

`$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$`


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-92-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 8. Pressupostos do modelo

___

Ao realizar uma regressão linear simples, devemos assumir/testar alguns pressupostos.

___

.pull-left[

1. O modelo linear descreve adequadamente a relação funcional entre `\(Y\)` e `\(X\)`;

2. Cada par de observação `\((y_i,x_i)\)` é independente dos demais;

3. A variável `\(X\)` é medida sem erros;

4. Os resíduos têm distribuição normal;

5. &lt;span style="color:#ed5858"&gt;A variância residual `\(\sigma^2\)` é constante ao longo de `\(X\)`.&lt;/span&gt;
]

.pull-right[

`$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$`


&lt;img src="Regressao_Linear_Simples_files/figure-html/unnamed-chunk-93-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: h1_small

# 9. Transformações lineares

&lt;h3 style="text-align:center"&gt;Um modelo de regressão linear &lt;b&gt;NÃO&lt;/b&gt; precisa ser uma linha reta&lt;/h3&gt;

&gt; O que define um modelo estatístico como linear é a posição dos seus parâmetros com relação a(s) variável(is) preditora(s). Os parametros a serem estimados devem estar na **MESMA LINHA** da variável dependente.

&gt; Os métodos discutidos para regressão linear simples também também se aplicam aos modelos de regressão múltipla e aos modelos polinomiais.

&lt;br&gt;&lt;/br&gt;
___

`\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)`: Regressão Linear Simples

`\(Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_p X_{ip} \epsilon_i\)`: Regressão Linear Múltipla

`\(Y_i = \beta_0 + \beta_1 X_{i} + \beta_2 X_i^2 \epsilon_i\)`: Regressão Polinomial

___

---

class: h1_small


# 9. Transformações lineares

#### Outros modelos podem ser *linearizados* por meio de uma **função de ligação** do tipo:

`$$\eta = g(\beta_iX_i)$$`

#### Modelos Lineares Generalizados 
___

`\(\eta = \beta_0 + \beta_1X_i\)`

`\(Y \sim \mathcal{N}(\mu = \beta_0 + \beta_1X_i, \sigma^2)\)`: Modelo Normal (ex. Regressão Linear Simples)

___

`\(\eta = log(\beta_0X_i^{\beta_1}) = log(\beta_0) + \beta_1log(X_i)\)`

`\(Y \sim \mathcal{Pois}(\lambda = e^{\eta})\)`: Modelo de Poisson (ex. variáveis de contagem)

___

`\(\eta = logit(\eta) = log\left(\frac{\eta}{1-\eta} \right)\)`

`\(Y \sim \mathcal{Binon}(n = 1, p = \frac{e^{\beta_0 + \beta_1X_i}}{1 + e^{\beta_0 + \beta_1X_i}})\)`: Modelo Binomial ou Regressão Logística (ex. variáveis categóricas binárias)

___

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "arta",
"highligthLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
